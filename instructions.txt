------------

KUBERNETES


Για την πρόσβαση στο VM που είναι ο master-node χρειάζεται VPN

1) Για να μπεις: ssh ubuntu@10.64.94.81 με pass tsou2303

2) Με το που μπεις τρέξε: sudo bash

3) Για να δεις αν υπάρχουν workers: kubectl get nodes 

4) Αν υπάρχουν (Είναι Ready) τότε κάνε skip το βήμα 5 και το βήμα 6

5) Κάνε load τους κόμβους με τα εξής images: (Διάλεξε όποιους κόμβους θες, αρκει ο κόμβος για το worker2 image να εχει usrp (με usb)
εγω διαλέγω παντα 55,56) 

omf load -t node055 -i /var/lib/omf-images-5.4/tsou-oai-worker1.ndz
omf load -t node056 -i /var/lib/omf-images-5.4/tsou-oai-worker2.ndz

6) Τσέκαρε αν έχουν γίνει ready μετά από το load με kubectl get nodes (μπορεί να αργήσουν λίγο να γίνουν ready)

7) Μπες στον worker2 (ssh root@node056): και τρέξε τις εξής εντολές για να ανοίξεις το usrp ( πρεπει να τις τρέξεις όλες και την τελευταία γιατί μπορεί να κολλήσει το cluster): 

sudo route add default gw 10.0.1.200 eth0
curl 10.1.0.56/usrpoff
curl 10.1.0.56/usrpon
sudo route delete default gw 10.0.1.200 eth0

8) Μπες στο directory: /home/ubuntu/oai-ml (cd oai-ml)
   Εκεί έχει όλα τα yaml files για το core, τα apps και τα du's. Eχει και κάποια scriptακια που είτε τα σηκώνουν όλα (./deploy-all)
   είτε σηκώνεις το core και το fronthaul (cu,du) ξεχωριστά (./deploy-core , ./deploy-front)

9) Αφού τα κάνεις όλα deploy μπορείς να δεις αν είναι όλα running με: kubectl get po -n oai . Αν είναι όλα running τότε μια χαρα.
   Επειδή κάποιες φορές σκάνε τα cu's , du's μπορείς να βλέπεις τα logs για το cu πχ με την εντολή kubectl logs -f oai-cu-7f88d687c5-pgwcv -n oai
   οπου oai-cu-7f88d687c5-pgwcv είναι το όνομα του pod (τυχαίο). Το ιδιο μπορείς να κάνεις και για τα pod του mme: (kubectl logs -f oai-mme-6789647dbd-kjltc -n oai mme) . Αν κάτι σκάσει ή εχει κολλήσει μπορείς να το κάνεις destroy με τα scripts: ./destroy-all , ./destroy-core , ./destroy-front



-------------

UE'S

Είναι δύσκολο να μπουν και τα 3 μαζί. Συνήθως ένα ue πετάει ένα άλλο. Αυτό όμως γίνεται συνήθως σε ένα ue.
Ένα workaround ειναι να κάνεις load 4 κόμβους (το ένα ρεζέρβα). Δοκιμασμένα με τη σειρά ue: node054, node057, node076, node058 και αν ενα πεταχτεί τότε βάλε το τελευταίο. 
π.χ: Η σειρα 54,57,76 παιζει τις περισσοτερες φορές με την μια και μπαινουν και τα 3 μαζί. Αν βγεί ένα βάλε το 58.
 

1) Image: app-aware-ue.ndz ( omf load -t node054 -i /var/lib/omf-images-5.4/app-aware-ue.ndz.ndz )
   ssh -X slicename@nitlab3.inf.uth.gr
   ssh -X node054

2)Για να συνδεθείς στο δίκτυο:

lte_dongle -o
 
minicom -D /dev/ttyUSB1

First run: at+cgdont?

It must return the following:

+CGDCONT: 0,"IP","apn.oai.svc.cluster.local","",0,0,0,0
+CGDCONT: 1,"IP","apn.oai.svc.cluster.local","",0,0,0,0

If not then try: (For easy copy paste: ^az -> t -> f -> change from 0 to 1 -> enter -> enter)
at+cgdcont=0,"IP","apn.oai.svc.cluster.local"
at+cgdcont=1,"IP","apn.oai.svc.cluster.local"

Check again: at+cgdont?


If it's ok , then:
at^ndisdup=1,1

Finally try:
at^dhcp?

If it returns IP then exit minicom and run:
./attach-ip.sh

Check if you can ping core ( ping 192.168.3.100 )

3) Change the default routes for each UE:

   sudo route delete default gw 10.0.1.200 eth0
   sudo route add default gw 192.168.20.1 wwan0



4) Για να δημιουργήσεις traffic με τα apps στο core: 

-- For WebRTC: sudo chromium-browser 192.168.3.101:8000 --no-sandbox &  (background run)
   kill: pkill chromium

-- For Sipp Client: sudo sipp -sn uac 192.168.3.102:1234
   kill: pkill sipp 

-- For Http Packets: curl -H 'Cache-Control: no-cache' 192.168.3.103:80
(for multiple requests watch -n 2 curl -H 'Cache-Control: no-cache' 192.168.3.103:80)




--------------------------

KUBEFLOW

An uparxei provlima me ta notebook servers (StatefulSets and Deployments fail to start):
Ston master:

kubectl delete mutatingwebhookconfigurations.admissionregistration.k8s.io --all
kubectl delete validatingwebhookconfigurations.admissionregistration.k8s.io --all

